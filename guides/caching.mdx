---
title: 'Caching Strategies'
description: 'Optimize performance and reduce costs with effective caching'
---

# Caching Strategies

Caching is essential for production RAG applications. This guide covers caching strategies to optimize performance and reduce API costs.

## Why Cache?

**Benefits:**
- âš¡ **Faster responses** - Serve cached results in milliseconds
- ðŸ’° **Lower costs** - Reduce API calls to FLTR and LLM providers
- ðŸŽ¯ **Better UX** - Instant answers for common questions
- ðŸ“ˆ **Higher throughput** - Handle more requests with same infrastructure

**Example savings:**
```
Without caching:
- 10,000 queries/day Ã— $0.001/query = $10/day = $3,650/year

With 80% cache hit rate:
- 2,000 queries/day Ã— $0.001/query = $2/day = $730/year
- Savings: $2,920/year (80% reduction)
```

## Caching Layers

### 1. Application-Level Cache

Cache search results and generated answers in your application.

#### Python (Redis)

```python
import redis
import json
import hashlib
from datetime import timedelta

# Redis client
cache = redis.Redis(host='localhost', port=6379, decode_responses=True)

def get_cache_key(query: str, dataset_id: str) -> str:
    """Generate cache key from query and dataset."""
    content = f"{dataset_id}:{query.lower().strip()}"
    return f"fltr:query:{hashlib.md5(content.encode()).hexdigest()}"

def cached_query(dataset_id: str, query: str) -> dict:
    """Query with caching."""
    cache_key = get_cache_key(query, dataset_id)

    # Try cache first
    cached = cache.get(cache_key)
    if cached:
        print("Cache HIT")
        return json.loads(cached)

    print("Cache MISS")

    # Fetch from FLTR
    results = client.query(
        dataset_id=dataset_id,
        query=query,
        limit=5
    )

    # Cache for 1 hour
    cache.setex(
        cache_key,
        timedelta(hours=1),
        json.dumps(results)
    )

    return results
```

#### Python (In-Memory LRU)

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_query_lru(query_hash: str, dataset_id: str, limit: int) -> str:
    """LRU cache for queries (up to 1000 entries)."""
    results = client.query(
        dataset_id=dataset_id,
        query=query_hash,  # Use original query, not hash
        limit=limit
    )
    # Return JSON string for caching
    return json.dumps(results)

def query_with_lru(dataset_id: str, query: str, limit: int = 5) -> dict:
    # Hash query for cache key (but use original for API)
    query_hash = hashlib.md5(query.lower().encode()).hexdigest()

    # This is cached
    results_json = cached_query_lru(query_hash, dataset_id, limit)

    return json.loads(results_json)
```

#### JavaScript (Redis)

```javascript
import Redis from 'ioredis';
import crypto from 'crypto';

const redis = new Redis({
  host: 'localhost',
  port: 6379
});

function getCacheKey(query, datasetId) {
  const content = `${datasetId}:${query.toLowerCase().trim()}`;
  return `fltr:query:${crypto.createHash('md5').update(content).digest('hex')}`;
}

async function cachedQuery(datasetId, query) {
  const cacheKey = getCacheKey(query, datasetId);

  // Try cache
  const cached = await redis.get(cacheKey);
  if (cached) {
    console.log('Cache HIT');
    return JSON.parse(cached);
  }

  console.log('Cache MISS');

  // Fetch from FLTR
  const results = await fltrClient.query({
    dataset_id: datasetId,
    query,
    limit: 5
  });

  // Cache for 1 hour (3600 seconds)
  await redis.setex(cacheKey, 3600, JSON.stringify(results));

  return results;
}
```

### 2. CDN/Edge Cache

Cache responses at the edge for global low-latency access.

#### Cloudflare Workers

```javascript
// worker.js
export default {
  async fetch(request, env) {
    const url = new URL(request.url);

    // Only cache GET requests
    if (request.method !== 'GET') {
      return fetch(request);
    }

    // Check cache
    const cache = caches.default;
    let response = await cache.match(request);

    if (response) {
      console.log('Edge cache HIT');
      return response;
    }

    console.log('Edge cache MISS');

    // Fetch from origin
    response = await fetch(request);

    // Cache successful responses for 1 hour
    if (response.ok) {
      const clonedResponse = response.clone();
      const headers = new Headers(clonedResponse.headers);
      headers.set('Cache-Control', 'public, max-age=3600');

      const cachedResponse = new Response(clonedResponse.body, {
        status: clonedResponse.status,
        statusText: clonedResponse.statusText,
        headers
      });

      await cache.put(request, cachedResponse);
    }

    return response;
  }
};
```

#### Vercel Edge Config

```typescript
// api/query.ts
import { get } from '@vercel/edge-config';
import { createHash } from 'crypto';

export const config = {
  runtime: 'edge',
};

export default async function handler(req: Request) {
  const { query, dataset_id } = await req.json();

  // Generate cache key
  const cacheKey = createHash('md5')
    .update(`${dataset_id}:${query}`)
    .digest('hex');

  // Check Edge Config cache
  const cached = await get(cacheKey);
  if (cached) {
    return new Response(JSON.stringify(cached), {
      headers: { 'Content-Type': 'application/json' }
    });
  }

  // Fetch from FLTR
  const results = await fltrClient.query({ dataset_id, query });

  // Cache in Edge Config (separate deployment needed)
  // await set(cacheKey, results);

  return new Response(JSON.stringify(results), {
    headers: { 'Content-Type': 'application/json' }
  });
}
```

### 3. Database Cache

Store frequently accessed results in your database.

```python
# models.py
from sqlalchemy import Column, String, JSON, DateTime, Integer
from datetime import datetime, timedelta

class QueryCache(Base):
    __tablename__ = 'query_cache'

    id = Column(Integer, primary_key=True)
    cache_key = Column(String, unique=True, index=True)
    dataset_id = Column(String, index=True)
    query = Column(String)
    results = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)
    hit_count = Column(Integer, default=0)
    expires_at = Column(DateTime)

# cache.py
def cached_query_db(session, dataset_id: str, query: str) -> dict:
    cache_key = get_cache_key(query, dataset_id)

    # Try database cache
    cached = session.query(QueryCache).filter(
        QueryCache.cache_key == cache_key,
        QueryCache.expires_at > datetime.utcnow()
    ).first()

    if cached:
        # Increment hit counter
        cached.hit_count += 1
        session.commit()
        return cached.results

    # Fetch from FLTR
    results = client.query(dataset_id=dataset_id, query=query)

    # Store in database
    cache_entry = QueryCache(
        cache_key=cache_key,
        dataset_id=dataset_id,
        query=query,
        results=results,
        expires_at=datetime.utcnow() + timedelta(hours=24)
    )
    session.add(cache_entry)
    session.commit()

    return results
```

## Cache Invalidation

### Time-Based Expiration

```python
# Short TTL for frequently changing data
cache.setex(cache_key, timedelta(minutes=5), json.dumps(results))

# Longer TTL for stable data
cache.setex(cache_key, timedelta(days=7), json.dumps(results))

# Infinite cache (manual invalidation only)
cache.set(cache_key, json.dumps(results))
```

### Event-Based Invalidation

```python
def invalidate_dataset_cache(dataset_id: str):
    """Invalidate all cache entries for a dataset."""
    # Get all keys matching pattern
    pattern = f"fltr:query:*:{dataset_id}:*"
    keys = cache.keys(pattern)

    if keys:
        cache.delete(*keys)
        print(f"Invalidated {len(keys)} cache entries")

# Invalidate when dataset updated
@app.post("/v1/datasets/{dataset_id}/documents")
async def upload_document(dataset_id: str, file: UploadFile):
    # Upload document
    document = await fltr.upload(dataset_id, file)

    # Invalidate cache
    invalidate_dataset_cache(dataset_id)

    return document
```

### LRU Eviction

```python
from cachetools import LRUCache

# LRU cache with max 1000 entries
cache = LRUCache(maxsize=1000)

def cached_query_lru(dataset_id: str, query: str) -> dict:
    cache_key = get_cache_key(query, dataset_id)

    if cache_key in cache:
        return cache[cache_key]

    results = client.query(dataset_id=dataset_id, query=query)

    # Store in LRU cache (automatically evicts least recently used)
    cache[cache_key] = results

    return results
```

## Cache Warming

Pre-populate cache with frequently asked questions.

```python
def warm_cache(dataset_id: str, common_questions: list[str]):
    """Pre-populate cache with common questions."""
    print(f"Warming cache for dataset {dataset_id}...")

    for i, question in enumerate(common_questions):
        # Query and cache
        results = cached_query(dataset_id, question)
        print(f"Cached question {i+1}/{len(common_questions)}: {question[:50]}...")

    print("Cache warming complete")

# Usage
common_questions = [
    "How do I reset my password?",
    "What are the pricing plans?",
    "How do I contact support?",
    "What file formats are supported?",
    "How do I delete my account?"
]

warm_cache("ds_abc123", common_questions)
```

## Advanced Patterns

### Multi-Tier Caching

```python
class MultiTierCache:
    """Three-tier cache: Memory â†’ Redis â†’ Database."""

    def __init__(self):
        self.memory = LRUCache(maxsize=100)  # L1: Fast, small
        self.redis = redis.Redis()            # L2: Medium, larger
        # self.db via SQLAlchemy                # L3: Slow, unlimited

    def get(self, key: str) -> dict | None:
        # Try L1 (memory)
        if key in self.memory:
            print("L1 cache HIT (memory)")
            return self.memory[key]

        # Try L2 (Redis)
        cached = self.redis.get(key)
        if cached:
            print("L2 cache HIT (Redis)")
            result = json.loads(cached)
            # Promote to L1
            self.memory[key] = result
            return result

        # Try L3 (database) - omitted for brevity

        print("Cache MISS")
        return None

    def set(self, key: str, value: dict, ttl: int = 3600):
        # Write to all tiers
        self.memory[key] = value
        self.redis.setex(key, ttl, json.dumps(value))
        # Write to database - omitted

cache = MultiTierCache()

def cached_query_multitier(dataset_id: str, query: str) -> dict:
    cache_key = get_cache_key(query, dataset_id)

    # Try cache
    cached = cache.get(cache_key)
    if cached:
        return cached

    # Fetch from FLTR
    results = client.query(dataset_id=dataset_id, query=query)

    # Store in cache
    cache.set(cache_key, results)

    return results
```

### Semantic Cache

Cache based on semantic similarity, not exact matches.

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class SemanticCache:
    """Cache that returns results for semantically similar queries."""

    def __init__(self, threshold: float = 0.95):
        self.cache = {}  # {embedding: (query, results)}
        self.threshold = threshold

    def _get_embedding(self, text: str) -> np.ndarray:
        """Get embedding for text (using FLTR or OpenAI)."""
        # Use FLTR's embedding model or OpenAI
        response = openai.Embedding.create(
            model="text-embedding-ada-002",
            input=text
        )
        return np.array(response['data'][0]['embedding'])

    def get(self, query: str) -> dict | None:
        """Get cached result for semantically similar query."""
        query_embedding = self._get_embedding(query)

        # Check all cached embeddings
        for cached_embedding, (cached_query, results) in self.cache.items():
            similarity = cosine_similarity(
                [query_embedding],
                [cached_embedding]
            )[0][0]

            if similarity >= self.threshold:
                print(f"Semantic cache HIT: {similarity:.3f} similarity to '{cached_query}'")
                return results

        return None

    def set(self, query: str, results: dict):
        """Cache results with query embedding."""
        query_embedding = self._get_embedding(query)
        self.cache[query_embedding.tobytes()] = (query, results)

semantic_cache = SemanticCache(threshold=0.95)

def cached_query_semantic(dataset_id: str, query: str) -> dict:
    # Try semantic cache
    cached = semantic_cache.get(query)
    if cached:
        return cached

    # Fetch from FLTR
    results = client.query(dataset_id=dataset_id, query=query)

    # Store in semantic cache
    semantic_cache.set(query, results)

    return results
```

### Probabilistic Caching

Use bloom filters to check cache existence before lookup.

```python
from pybloom_live import BloomFilter

class ProbabilisticCache:
    """Cache with bloom filter for fast existence checks."""

    def __init__(self, capacity: int = 10000):
        self.bloom = BloomFilter(capacity=capacity, error_rate=0.001)
        self.cache = {}

    def get(self, key: str) -> dict | None:
        # Fast check: definitely not in cache
        if key not in self.bloom:
            return None

        # Might be in cache (could be false positive)
        return self.cache.get(key)

    def set(self, key: str, value: dict):
        self.bloom.add(key)
        self.cache[key] = value

prob_cache = ProbabilisticCache()
```

## Monitoring Cache Performance

### Metrics to Track

```python
import time
from dataclasses import dataclass
from typing import Optional

@dataclass
class CacheMetrics:
    hits: int = 0
    misses: int = 0
    total_queries: int = 0
    total_latency_saved: float = 0.0  # milliseconds

    @property
    def hit_rate(self) -> float:
        if self.total_queries == 0:
            return 0.0
        return self.hits / self.total_queries

    @property
    def avg_latency_saved(self) -> float:
        if self.hits == 0:
            return 0.0
        return self.total_latency_saved / self.hits

metrics = CacheMetrics()

def cached_query_with_metrics(dataset_id: str, query: str) -> dict:
    cache_key = get_cache_key(query, dataset_id)
    metrics.total_queries += 1

    # Try cache
    start = time.time()
    cached = cache.get(cache_key)
    cache_latency = (time.time() - start) * 1000  # ms

    if cached:
        metrics.hits += 1
        # Assume API call would take ~200ms
        metrics.total_latency_saved += (200 - cache_latency)
        print(f"Cache HIT ({cache_latency:.1f}ms)")
        return json.loads(cached)

    metrics.misses += 1

    # Fetch from FLTR
    start = time.time()
    results = client.query(dataset_id=dataset_id, query=query)
    api_latency = (time.time() - start) * 1000  # ms

    print(f"Cache MISS (API: {api_latency:.1f}ms)")

    # Cache results
    cache.setex(cache_key, 3600, json.dumps(results))

    return results

# Print metrics periodically
def print_cache_metrics():
    print(f"""
    Cache Metrics:
    - Hit Rate: {metrics.hit_rate:.1%}
    - Total Queries: {metrics.total_queries}
    - Hits: {metrics.hits}
    - Misses: {metrics.misses}
    - Avg Latency Saved: {metrics.avg_latency_saved:.1f}ms
    - Total Latency Saved: {metrics.total_latency_saved / 1000:.1f}s
    """)
```

## Best Practices

### âœ… Do's

- **Cache immutable data** - Documents, embeddings, search results
- **Use appropriate TTL** - Shorter for dynamic data, longer for static
- **Monitor hit rate** - Aim for >70% hit rate
- **Warm cache proactively** - Pre-cache common queries
- **Use cache keys wisely** - Normalize queries (lowercase, trim)
- **Invalidate on updates** - Clear cache when data changes
- **Set memory limits** - Use LRU or similar eviction policy

### âŒ Don'ts

- **Don't cache user-specific data** - Unless properly keyed by user
- **Don't cache forever** - Always set TTL
- **Don't cache errors** - Only cache successful results
- **Don't over-cache** - Balance memory usage and hit rate
- **Don't ignore cache size** - Monitor and limit growth
- **Don't cache in development** - Makes debugging harder

## Cost Analysis

### Example: 10,000 queries/day

**Without caching:**
```
FLTR API:  10,000 Ã— $0.001 = $10/day
LLM calls: 10,000 Ã— $0.010 = $100/day
Total:     $110/day = $40,150/year
```

**With 80% cache hit rate:**
```
FLTR API:  2,000 Ã— $0.001 = $2/day
LLM calls: 2,000 Ã— $0.010 = $20/day
Redis:     $15/month = $0.50/day
Total:     $22.50/day = $8,212/year

Savings:   $31,938/year (79.5% reduction)
```

## Production Checklist

- [ ] Implement caching layer (Redis, Memcached, or in-memory)
- [ ] Set appropriate TTL for different data types
- [ ] Monitor cache hit rate (target >70%)
- [ ] Implement cache invalidation on data updates
- [ ] Warm cache with common queries
- [ ] Set memory limits and eviction policy
- [ ] Log cache metrics (hits, misses, latency)
- [ ] Set up alerts for low hit rate
- [ ] Document cache key format
- [ ] Test cache invalidation logic

## Additional Resources

<CardGroup cols={2}>
  <Card title="RAG Best Practices" icon="robot" href="/guides/rag-best-practices">
    Build production RAG apps
  </Card>

  <Card title="Rate Limits" icon="gauge" href="/support/rate-limits">
    Understand API rate limits
  </Card>

  <Card title="Performance" icon="bolt" href="/support/troubleshooting">
    Optimize query performance
  </Card>

  <Card title="MCP API" icon="magnifying-glass" href="/api-reference/mcp/query">
    MCP query endpoint
  </Card>
</CardGroup>

## Questions?

Need help implementing caching? Contact our support team.

**Support:** support@fltr.com
